# private-Ai
developed a local Retrieval Augmented Generation (RAG) pipeline enabling conversational interactions with PDF files offline. This pipeline seamlessly integrates a retrieval component with the Mistral Llama model, facilitating coherent and context-aware responses. The process involves loading a PDF file using the UnstructuredPDFLoader, splitting it into chunks with the RecursiveCharacterTextSplitter, and creating embeddings using OllamaEmbeddings. Leveraging the Chroma library's 'from_documents' method, a new vector database is generated, incorporating updated chunks and Ollama embeddings. The model retrieves relevant context from this database, generates an answer based on both context and the question, and returns the parsed output.
